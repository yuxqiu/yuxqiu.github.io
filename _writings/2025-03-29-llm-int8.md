---
layout: post
title: "LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale"
date: 2025-04-25
tags: ["AI"]

featured: false
katex: false
---

<div class="links">
    <a href="/assets/pdf/writings/2025/llm.int8.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Slide</a>
</div>

## Abstract

In this presentation, I reviewed the paper [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://dl.acm.org/doi/10.5555/3600270.3602468). The discussion encompassed various quantization techniques, the emergence of outlier features in large language models, and the evaluation results reported in the study.

In addition to summarizing the original work, I corrected an erratum related to zero-point quantization and demonstrated how the LLM.int8 workflow is practically integrated with [Hugging Face's ecosystem](https://huggingface.co/blog/hf-bitsandbytes-integration).